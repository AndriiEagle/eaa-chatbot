import { OpenAI } from 'openai';
import type { ChatCompletionMessageParam } from 'openai/resources/chat/completions';
import { env, EMBEDDING_MODEL, CHAT_MODEL } from '../config/environment.js';

// –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç OpenAI
export const openai = new OpenAI({ apiKey: env.OPENAI_API_KEY });

// === –ö–≠–® –≠–ú–ë–ï–î–î–ò–ù–ì–û–í (in-memory) ===
const embeddingCache = new Map<string, number[]>();

export function clearEmbeddingCache() {
  embeddingCache.clear();
}

/**
 * –°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ (embedding) –¥–ª—è —Ç–µ–∫—Å—Ç–∞
 * @param text - –¢–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
 * @returns –ú–∞—Å—Å–∏–≤ —á–∏—Å–µ–ª, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–π —ç–º–±–µ–¥–¥–∏–Ω–≥
 * @throws –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—É—Å—Ç–æ–º —Ç–µ–∫—Å—Ç–µ –∏–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å API
 */
export async function createEmbedding(text: string): Promise<number[]> {
  if (!text || typeof text !== 'string' || text.trim().length === 0) {
    throw new Error('‚ùå [EMBEDDING] Text for embedding creation must be a non-empty string');
  }

  const cacheKey = text.trim();
  
  // Check cache first
  if (embeddingCache.has(cacheKey)) {
    console.log('üéØ [EMBEDDING] Cache hit');
    return embeddingCache.get(cacheKey)!;
  }

  try {
    console.log(`üîÑ [EMBEDDING] Creating embedding for text (${text.length} chars)...`);
    
    const response = await openai.embeddings.create({
      model: EMBEDDING_MODEL,
      input: text.trim(),
    });

    const embedding = response.data[0]?.embedding;
    
    if (!embedding || embedding.length === 0) {
      throw new Error('‚ùå [EMBEDDING] API returned empty embedding');
    }

    // Cache the result
    embeddingCache.set(cacheKey, embedding);
    
    console.log(`‚úÖ [EMBEDDING] Created embedding with ${embedding.length} dimensions`);
    return embedding;
    
  } catch (error) {
    console.error('‚ùå [EMBEDDING] Error creating embedding:', error);
    throw error;
  }
}

/**
 * –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –≤–æ–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
 * @param messages - –ú–∞—Å—Å–∏–≤ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ API
 * @param temperature - –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (randomness) –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0
 * @returns –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
 * @throws –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—É—Å—Ç–æ–º –º–∞—Å—Å–∏–≤–µ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å API
 */
export async function generateCompletion(
  messages: ChatCompletionMessageParam[],
  temperature: number = 0
): Promise<string> {
  if (!messages || messages.length === 0) {
    throw new Error('‚ùå [GENERATE] Message array must not be empty');
  }

  try {
    console.log(`ü§ñ [GENERATE] Generating completion with ${messages.length} messages...`);
    
    const completion = await openai.chat.completions.create({
      model: CHAT_MODEL,
      messages,
      temperature,
      max_tokens: 2000,
    });

    const content = completion.choices[0]?.message?.content;
    
    if (!content) {
      console.warn('‚ö†Ô∏è [GENERATE] Empty response from OpenAI');
      return '';
    }

    console.log(`‚úÖ [GENERATE] Generated completion (${content.length} chars)`);
    return content;
    
  } catch (error) {
    console.error('‚ùå [GENERATE] Error generating completion:', error);
    throw error;
  }
}

/**
 * –°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ-–ø–æ—Ç–æ–∫–æ–≤—ã–π –æ—Ç–≤–µ—Ç
 * @param messages - –ú–∞—Å—Å–∏–≤ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ API
 * @param temperature - –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0
 * @returns –°—Ç—Ä–∏–º –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ –æ—Ç–≤–µ—Ç–∞
 * @throws –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—É—Å—Ç–æ–º –º–∞—Å—Å–∏–≤–µ —Å–æ–æ–±—â–µ–Ω–∏–π –∏–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å API
 */
export async function generateStreamingCompletion(
  messages: ChatCompletionMessageParam[],
  temperature: number = 0
) {
  if (!messages || messages.length === 0) {
    throw new Error('‚ùå [STREAM] Message array must not be empty');
  }

  try {
    console.log(`üåä [STREAM] Starting streaming completion with ${messages.length} messages...`);
    
    const stream = await openai.chat.completions.create({
      model: CHAT_MODEL,
      messages,
      temperature,
      max_tokens: 2000,
      stream: true,
    });

    console.log('‚úÖ [STREAM] Streaming started successfully');
    return stream;
    
  } catch (error) {
    console.error('‚ùå [STREAM] Error starting streaming completion:', error);
    throw error;
  }
} 